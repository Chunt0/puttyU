{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup, Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin by getting a list of all artist urls\n",
    "MAX_ARTISTS = 6215\n",
    "top_path = \"https://tzvetnik.online/articles/artist/\"\n",
    "\n",
    "artist_urls = [top_path+str(i+1) for i in range(MAX_ARTISTS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step two: get all the show urls\n",
    "top_path = \"https://tzvetnik.online\"\n",
    "show_urls = []\n",
    "for url in tqdm(artist_urls):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    try:\n",
    "        response.raise_for_status() # Raises an exception for HTTP errors\n",
    "        artist_html = response.text\n",
    "        soup = BeautifulSoup(artist_html, 'html.parser')\n",
    "\n",
    "        article_previews = soup.find('div', class_=\"article-previews\")\n",
    "        anchor_tags = article_previews.find_all('a')\n",
    "           \n",
    "        for anchor_tag in anchor_tags:\n",
    "            href_value = top_path + anchor_tag.get('href')\n",
    "            show_urls.append(href_value)       \n",
    "\n",
    "    except:\n",
    "        print(f\"error requesting {url} | status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "show_urls = list(set(show_urls))\n",
    "with open(\"show_urls.txt\", \"w\") as file:\n",
    "    for item in show_urls:\n",
    "        file.write(item+\"\\n\")\n",
    "len(show_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2917\n",
      "2917\n"
     ]
    }
   ],
   "source": [
    "with open(\"show_urls.txt\", \"r\") as file:\n",
    "    urls = [line.strip() for line in file.readlines()]\n",
    "print(len(urls))\n",
    "urls = list(set(urls))\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e680f368a14b47ad93856b28cc9fbf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2917 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error has occured for https://tzvetnik.online/articles/artist/2646?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1525?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/77?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/2100?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/922?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/877?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/910?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/966?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1023?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1715?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1060?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/2882?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/623?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/2785?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/855?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/2335?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1116?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/2507?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1028?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1150?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/3005?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/2417?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1710?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/572?page=2 | status code: 200\n",
      "error has occured for https://tzvetnik.online/articles/artist/1811?page=2 | status code: 200\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Data structure format:\n",
    "data = [show, ...]\n",
    "show = {\"show_url\" : \"show_url\",\n",
    "        \"dir\" : \"dir\",\n",
    "        \"img_urls\" = [\"img_url\", \"caption\"], # this is different from CAD, CAD json might need to be altered - we need \"caption\" for labeling purposes\n",
    "        \"press_release_url\" = \"any text found on page, this will likely need to be concatinated together\", # this one is tricky because i want the same format between websites but each website is different, for tzvetnik the press release is just text on the page, unlike CAD\n",
    "        \"artist\" : \"artist\",\n",
    "        \"show_title\" : \"show_title\",\n",
    "        \"venue\" : \"venue\"}\n",
    "\n",
    "\"\"\"\n",
    "data = []\n",
    "for url in tqdm(urls):\n",
    "    dir = os.path.basename(url)\n",
    "    show = {\"show_url\":url, \"dir\": dir}\n",
    "    img_urls = []\n",
    "\n",
    "    # Fetch html\n",
    "    response = requests.get(url)\n",
    "\n",
    "    try:\n",
    "        response.raise_for_status()\n",
    "        show_html = response.text\n",
    "        soup = BeautifulSoup(show_html, \"html.parser\")\n",
    "\n",
    "        # Gather all img_urls and their associated captions\n",
    "        # All content is contained in various tags in <form class=\"article-form\" >\n",
    "        article_form = soup.find(\"form\", class_=\"article-form\")\n",
    "        action_texts = soup.find_all(\"action-text-attachment\")\n",
    "        for action_text in action_texts:\n",
    "            try:\n",
    "                img_tag = action_text.find(\"img\")\n",
    "                if img_tag == None:\n",
    "                    pass\n",
    "                else:\n",
    "                    img_url = img_tag.get(\"src\")\n",
    "                    caption = action_text.get(\"caption\")\n",
    "                    img_urls.append((img_url, caption))\n",
    "            except:\n",
    "                print(f\"an error occured retrieving img_url and caption for url: {url}\")\n",
    "        show[\"img_urls\"] = img_urls\n",
    "        \n",
    "        # Grab the press realease/text -- this was stupid to figure out. i bet there is a better way...\n",
    "        press_release = \"\"\n",
    "        trix_content = soup.find(\"div\", class_=\"trix-content\")\n",
    "        divs = [div for div in trix_content.find('div') if isinstance(div, Tag)]\n",
    "        press_release = []\n",
    "        for div in divs:\n",
    "            new_divs = div.find_all(\"div\")\n",
    "            for d in new_divs:\n",
    "                if \"<br/\" in str(d):\n",
    "                   d = d.text\n",
    "                   press_release.append(d)\n",
    "        show[\"press_release_pdf\"] = \"\".join(press_release).strip()\n",
    "\n",
    "        # Grab artist names and venue\n",
    "        article_show = article_form.find(\"div\", class_=\"article__tags article--show\")\n",
    "        hrefs = article_show.find_all('a')\n",
    "        artist = []\n",
    "        venue = []\n",
    "        for a_tag in hrefs:\n",
    "            href = a_tag.get(\"href\").split('/')\n",
    "            if 'artist' == href[2]:\n",
    "                artist.append(a_tag.text)\n",
    "            else:\n",
    "                venue.append(a_tag.text)\n",
    "        show[\"artist\"] = artist\n",
    "        show[\"venue\"] = venue\n",
    "\n",
    "        # Grab show title\n",
    "        show_title_tag = article_form.find(\"h1\", class_=\"article--show\")\n",
    "        show[\"title\"] = show_title_tag.text.strip()\n",
    "\n",
    "    \n",
    "    except:\n",
    "        print(f\"error has occured for {url} | status code: {response.status_code}\")\n",
    "    data.append(show)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [d for d in data if len(d.items()) == 7]\n",
    "\n",
    "json_filename = \"tzvet_data.json\"\n",
    "\n",
    "with open(json_filename, \"w\") as json_file:\n",
    "    json.dump(new_data, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This section must be fixed so that the data is downloaded in the proper\n",
    "# ImageFolder format, see finetuning0.ipynb for details and helpful links.\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to download a single image\n",
    "def download_image(img_url, img_path):\n",
    "    try:\n",
    "        response = requests.get(img_url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Check if the image file already exists, skip if it does\n",
    "        if not os.path.exists(img_path):\n",
    "            with open(img_path, 'wb') as img_file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    img_file.write(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {img_url}: {e}\")\n",
    "\n",
    "# Create the 'img' directory if it doesn't exist\n",
    "img_directory = 'img'\n",
    "os.makedirs(img_directory, exist_ok=True)\n",
    "\n",
    "# Loop through the list of dictionaries\n",
    "#for item in tqdm(data):\n",
    "    dir_name = item['dir']\n",
    "    img_urls = item['imgs']\n",
    "\n",
    "    # Create a subdirectory within 'img' based on 'dir' value\n",
    "    subdirectory_path = os.path.join(img_directory, dir_name)\n",
    "    os.makedirs(subdirectory_path, exist_ok=True)\n",
    "\n",
    "    # Create a ThreadPoolExecutor with a maximum of 5 threads (adjust as needed)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "\n",
    "        # Download and save images in parallel\n",
    "        for img_url in tqdm(img_urls):\n",
    "            img_filename = os.path.basename(img_url[0])\n",
    "            img_path = os.path.join(subdirectory_path, img_filename)\n",
    "\n",
    "            futures.append(executor.submit(download_image, img_url[0], img_path))\n",
    "\n",
    "        # Wait for all download tasks to complete\n",
    "        for future in futures:\n",
    "            future.result()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
