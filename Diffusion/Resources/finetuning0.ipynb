{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning A PreTrained Stable Diffusion Model\n",
    "\n",
    "The process of fine tuning a Stable Diffusion model can be broken down into these 14 core steps:\n",
    "\n",
    "1. **Data Collection and Preprocessing**:\n",
    "   - Collect and preprocess the dataset relevant to your task. Ensure that the data is cleaned, organized, and structured in a format that can be used for training. For image-related tasks, you may need labeled image data.\n",
    "\n",
    "\n",
    "2. **Initialize the Pre-trained Model**:\n",
    "   - Start with a pre-trained stable diffusion model like DALL-E, which has already been trained on a large dataset. Initialize the model using the pre-trained weights.\n",
    "\n",
    "3. **Define the Task**:\n",
    "   - Clearly define the task or objective for fine-tuning. For example, if you are adapting DALL-E for generating specific types of images, specify what those images should look like and provide relevant metadata or labels.\n",
    "\n",
    "4. **Adjust Model Architecture**:\n",
    "   - Modify the model architecture if necessary. You may need to add or remove layers, change the model's size, or make other architectural adjustments to suit your task.\n",
    "\n",
    "5. **Data Loading and Augmentation**:\n",
    "   - Implement data loading and augmentation techniques that are appropriate for your task. This is crucial, especially for image-related tasks. Data augmentation can help increase the diversity of your training data.\n",
    "\n",
    "6. **Loss Function Selection**:\n",
    "   - Choose an appropriate loss function for your task. This could be a standard loss function like mean squared error for regression tasks or a more complex loss function tailored to your specific problem.\n",
    "\n",
    "7. **Hyperparameter Tuning**:\n",
    "   - Fine-tune hyperparameters such as learning rate, batch size, and the number of training epochs. You may need to experiment with different hyperparameters to achieve the best performance.\n",
    "\n",
    "8. **Training**:\n",
    "   - Train the model on your fine-tuning dataset using the adjusted architecture, data loading, augmentation, and loss function. Monitor training progress, evaluate performance on a validation set, and use techniques like early stopping to prevent overfitting.\n",
    "\n",
    "9. **Regularization**:\n",
    "   - Apply regularization techniques such as dropout, weight decay, or layer normalization to prevent overfitting.\n",
    "\n",
    "10. **Evaluation**:\n",
    "    - Evaluate the fine-tuned model using appropriate evaluation metrics for your task. For image generation, this might involve generating images and assessing their quality. For other tasks, use relevant evaluation metrics.\n",
    "\n",
    "11. **Iterative Fine-tuning**:\n",
    "    - Fine-tuning is often an iterative process. Based on the evaluation results, make necessary adjustments to the model architecture, data, or hyperparameters, and repeat the training process until you achieve the desired performance.\n",
    "\n",
    "12. **Deployment and Inference**:\n",
    "    - Once the fine-tuned model meets your requirements, deploy it for inference in your application. Ensure that it works as expected in a production environment.\n",
    "\n",
    "13. **Monitoring and Maintenance**:\n",
    "    - Continuously monitor the model's performance in production, and retrain or fine-tune it as needed to adapt to changing data distributions or requirements.\n",
    "\n",
    "14. **Documentation**:\n",
    "    - Document the fine-tuning process, including the model architecture, hyperparameters, and any specific details related to your task. This documentation is valuable for future reference and collaboration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection and Preprocessings\n",
    "\n",
    "To begin this process we have to clearly state our objective.\n",
    "\n",
    "Goal:\n",
    "- Given a gallery press release (or painting title(s) or labels) produce `n` many images of contempary art\n",
    "- Use this model to simulate a contemporary art gallery\n",
    "\n",
    "For this task we will acquire our dataset from openly available contemporary art aggregator websites\n",
    "- https://www.contemporaryartlibrary.org/\n",
    "- https://tzvetnik.online/\n",
    "\n",
    "To collect the image files and meta data we will employ webscraping techniques utilizing `requests` to acquire html and `bs4` (BeautifulSoup) to parse the html and extract the necessary files.\n",
    "\n",
    "This data will be compiled into json format to then be used to download each individual file and organize them appropriately.\n",
    "\n",
    "Each website has it's on file structure that must be navigated. Each website may have more or less metadata tethered to each image or gallery show. These idiosyncracies will determine how the particular data scraping scripts will be executed. Methods for data extraction for each website can be found in `/fake_gallery`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ImageFolder\n",
    "\n",
    "To create a dataset that the PyTorch and Hugging Face libraries can comprehend we must construct them in accordance to the ImageFolder dataset guidelines.\n",
    "\n",
    "Here are some useful links to refer to if you get lost:\n",
    "- https://huggingface.co/docs/datasets/image_dataset#imagefolder\n",
    "- https://huggingface.co/docs/diffusers/v0.13.0/en/training/text2image\n",
    "- https://github.com/huggingface/diffusers/tree/main/examples/text_to_image\n",
    "- https://huggingface.co/docs/diffusers/tutorials/basic_training\n",
    "- https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb\n",
    "- https://huggingface.co/docs/datasets/dataset_script#create-a-dataset-loading-script\n",
    "\n",
    "From the Hugging Face Dataset documentation they claim there are two core methods for creating and sharing an image dataset\n",
    "* Using `ImageFolder` and some metadata (this is a no code solution)\n",
    "* Using a loading script.\n",
    "\n",
    "#### ImageFolder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image\n",
    "\n",
    "dataset = load_dataset(\"beans\", split=\"train\")\n",
    "dataset[500][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "- [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf)\n",
    "- [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2102.09672.pdf)\n",
    "- [Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/pdf/2105.05233.pdf)\n",
    "- [CLASSIFIER-FREE DIFFUSION GUIDANCE](https://arxiv.org/pdf/2207.12598.pdf)\n",
    "- [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/pdf/1503.03585.pdf)\n",
    "- [Positional Encoding in Transformer Models](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)\n",
    "- [A youtube video I haven't watched yet!](https://www.youtube.com/watch?v=TBCRlnwJtZU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
